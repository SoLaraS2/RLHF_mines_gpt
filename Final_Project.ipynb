{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from muutils.dictmagic import condense_tensor_dict\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import os  \n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Geoff McIntyre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: ugh (Easter Island) is situated on the mainland (with the North Pole and East Pole) between the Great Basin and the Arctic Circle. It is one of the few free or commercial shipping routes that will be open from here.\n",
      "\n",
      "In February of 2017, it was found by a National Transportation Safety Board helicopter that a portion of the cargo line was submerged in water. A total of 25\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "def generate_text(prompt, model):\n",
    "    \"\"\" Generate text using the specified model and prompt. \"\"\"\n",
    "    result = model(prompt, max_length=80, truncation=True)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"gpt2\"\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Get input from the user\n",
    "    user_input = input(\"Enter your prompt: \")\n",
    "    \n",
    "    # Generate and print the output text\n",
    "    output_text = generate_text(user_input, model)\n",
    "    print(\"Generated Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name, trust_remote_code=True)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        root.title(\"Model Comparison Tool\")\n",
    "        name1 = \"gpt2\"\n",
    "        name2 = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "        self.model1 = load_model(name1)  \n",
    "        self.model2 = load_model(name2)  \n",
    "\n",
    "        # Setup the UI\n",
    "        tk.Label(root, text=\"Enter your prompt:\").pack()\n",
    "        \n",
    "        self.prompt_entry = tk.Entry(root, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        self.generate_button = tk.Button(root, text=\"Generate Texts\", command=self.generate_texts)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack() \n",
    "\n",
    "        self.output1_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output1_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=2).pack() \n",
    "\n",
    "        self.output2_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output2_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack()\n",
    "\n",
    "        self.select_button1 = tk.Button(root, text=\"Select Output 1\", command=lambda: self.update_model(1))\n",
    "        self.select_button1.pack()\n",
    "        self.select_button1.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.select_button2 = tk.Button(root, text=\"Select Output 2\", command=lambda: self.update_model(2))\n",
    "        self.select_button2.pack()\n",
    "        self.select_button2.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.corgis_label = tk.Label(root, text=\"Press the 'X' once you have had enough fun.\")\n",
    "        self.corgis_label.pack(side=tk.BOTTOM)\n",
    "\n",
    "    def generate_texts(self):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        result1 = self.model1(prompt, max_length=80, truncation=True)\n",
    "        result2 = self.model2(prompt, max_length=80, truncation=True)\n",
    "        self.output1 = result1[0]['generated_text']\n",
    "        self.output2 = result2[0]['generated_text']\n",
    "\n",
    "        self.output1_label.config(text=self.output1)\n",
    "        self.output2_label.config(text=self.output2)\n",
    "\n",
    "        self.select_button1.config(state=\"normal\")\n",
    "        self.select_button2.config(state=\"normal\")\n",
    "        \n",
    "    def log_user_feedback(self, prompt, selected_output, model_name):\n",
    "        feedback_data = {\n",
    "            'prompt': prompt,\n",
    "            'selected_output': selected_output,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "\n",
    "        with open('user_feedback.jsonl', 'a', encoding='utf-8') as file:\n",
    "            json.dump(feedback_data, file)\n",
    "            file.write('\\n')  # Write a newline to separate JSON objects\n",
    "\n",
    "    def update_model(self, selected_model):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        selected_output = self.output1 if selected_model == 1 else self.output2\n",
    "        model_name = \"model1\" if selected_model == 1 else \"model2\"\n",
    "        \n",
    "        self.log_user_feedback(prompt, selected_output, model_name)\n",
    "        messagebox.showinfo(\"Selection\", \"Feedback recorded! Generating new texts...\")\n",
    "        self.generate_texts()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Geoff McIntyre\\MirrorLab\\RLHF_mines_gpt\\wandb\\run-20240424_221012-cle6dbmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laraeseuemail/just_work/runs/cle6dbmr' target=\"_blank\">running_ostrich</a></strong> to <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laraeseuemail/just_work/runs/cle6dbmr' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work/runs/cle6dbmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:39<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.3284, 'train_samples_per_second': 1.373, 'train_steps_per_second': 0.381, 'train_loss': 2.686314900716146, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>14109769728000.0</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train_loss</td><td>2.68631</td></tr><tr><td>train_runtime</td><td>39.3284</td></tr><tr><td>train_samples_per_second</td><td>1.373</td></tr><tr><td>train_steps_per_second</td><td>0.381</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">running_ostrich</strong> at: <a href='https://wandb.ai/laraeseuemail/just_work/runs/cle6dbmr' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work/runs/cle6dbmr</a><br/> View project at: <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240424_221012-cle6dbmr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "def fine_tune_model(model_name, jsonl_file, output_dir):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"just_work\", name=\"running_ostrich\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token if undefined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Make sure the model's embedding sizes match if you're changing the tokenizer's pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Load dataset from JSONL file\n",
    "    dataset = load_dataset('json', data_files=jsonl_file, split='train')\n",
    "    \n",
    "    # Define preprocessing function to concatenate prompt and selected_output\n",
    "    def preprocess_function(examples):\n",
    "        texts = [p + \" \" + o for p, o in zip(examples['prompt'], examples['selected_output'])]\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Map preprocessing function\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # Data collator for padding\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define a function to compute the loss and accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        accuracy = (preds == labels).float().mean()\n",
    "        return {\"loss\": loss.item(), \"accuracy\": accuracy.item()}\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        compute_metrics=compute_metrics  # Add the compute_metrics function\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "# Example usage:\n",
    "fine_tune_model('gpt2', 'user_feedback.jsonl', 'fine_tuned_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneratorApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Text Generator\")\n",
    "\n",
    "        # Entry widget to take user input for the prompt\n",
    "        self.prompt_entry = tk.Entry(master, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        # Button to trigger text generation\n",
    "        self.generate_button = tk.Button(master, text=\"Generate\", command=self.generate_text)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        # Label to display the generated text\n",
    "        self.result_label = tk.Label(master, text=\"\", wraplength=400)\n",
    "        self.result_label.pack()\n",
    "\n",
    "    def generate_text_from_prompt(self, prompt):\n",
    "        model_dir = 'fine_tuned_model'  \n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        encoding = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask'],\n",
    "            max_length=80,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "    def generate_text(self):\n",
    "        user_input = self.prompt_entry.get()\n",
    "        result = self.generate_text_from_prompt(user_input)\n",
    "        self.result_label.config(text=result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextGeneratorApp(root)\n",
    "    root.mainloop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
