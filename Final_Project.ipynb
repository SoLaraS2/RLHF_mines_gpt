{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from muutils.dictmagic import condense_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import os  # Ensure os is imported at the top\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        root.title(\"Model Comparison Tool\")\n",
    "\n",
    "        self.model1 = load_model(\"gpt2\")  \n",
    "        self.model2 = load_model('EleutherAI/gpt-neo-1.3B')  \n",
    "\n",
    "        # Setup the UI\n",
    "        tk.Label(root, text=\"Enter your prompt:\").pack()\n",
    "        \n",
    "        self.prompt_entry = tk.Entry(root, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        self.generate_button = tk.Button(root, text=\"Generate Texts\", command=self.generate_texts)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        self.output1_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output1_label.pack()\n",
    "\n",
    "        self.output2_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output2_label.pack()\n",
    "\n",
    "        self.select_button1 = tk.Button(root, text=\"Select Output 1\", command=lambda: self.update_model(1))\n",
    "        self.select_button1.pack()\n",
    "        self.select_button1.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.select_button2 = tk.Button(root, text=\"Select Output 2\", command=lambda: self.update_model(2))\n",
    "        self.select_button2.pack()\n",
    "        self.select_button2.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "    def generate_texts(self):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        result1 = self.model1(prompt, max_length=50, truncation=True)\n",
    "        result2 = self.model2(prompt, max_length=50, truncation=True)\n",
    "        self.output1 = result1[0]['generated_text']\n",
    "        self.output2 = result2[0]['generated_text']\n",
    "\n",
    "        self.output1_label.config(text=self.output1)\n",
    "        self.output2_label.config(text=self.output2)\n",
    "\n",
    "        self.select_button1.config(state=\"normal\")\n",
    "        self.select_button2.config(state=\"normal\")\n",
    "        \n",
    "    def log_user_feedback(self, prompt, selected_output, model_name):\n",
    "        # Check if the file exists to determine if headers are needed\n",
    "        file_exists = os.path.isfile('user_feedback.csv')\n",
    "        \n",
    "        with open('user_feedback.csv', 'a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write headers if the file is being created\n",
    "            if not file_exists:\n",
    "                writer.writerow(['prompt', 'selected_output', 'model_name'])\n",
    "            \n",
    "            # Write the actual data\n",
    "            writer.writerow([prompt, selected_output, model_name])\n",
    "\n",
    "    def update_model(self, selected_model):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        selected_output = self.output1 if selected_model == 1 else self.output2\n",
    "        model_name = \"model1\" if selected_model == 1 else \"model2\"\n",
    "        \n",
    "        self.log_user_feedback(prompt, selected_output, model_name)\n",
    "        messagebox.showinfo(\"Selection\", \"Feedback recorded! Generating new texts...\")\n",
    "        self.generate_texts()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 171.84 examples/s]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Geoff McIntyre\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Geoff McIntyre\\decoding-gpt\\notebooks\\wandb\\run-20240418_223406-btjw7c07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laraeseuemail/huggingface/runs/btjw7c07' target=\"_blank\">jumping-bee-1</a></strong> to <a href='https://wandb.ai/laraeseuemail/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laraeseuemail/huggingface' target=\"_blank\">https://wandb.ai/laraeseuemail/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laraeseuemail/huggingface/runs/btjw7c07' target=\"_blank\">https://wandb.ai/laraeseuemail/huggingface/runs/btjw7c07</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 84.5422, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.035, 'train_loss': 2.884645462036133, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "def fine_tune_model(model_name, csv_file, output_dir):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token if undefined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Make sure the model's embedding sizes match if you're changing the tokenizer's pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('csv', data_files=csv_file, split='train')\n",
    "    \n",
    "    # Define preprocessing function to concatenate prompt and selected_output\n",
    "    def preprocess_function(examples):\n",
    "        texts = [p + \" \" + o for p, o in zip(examples['prompt'], examples['selected_output'])]\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Map preprocessing function\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # Data collator for padding\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    # Initialize and run trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Call function to fine-tune model\n",
    "fine_tune_model('gpt2', 'user_feedback.csv', 'fine_tuned_model')\n",
    "\n",
    "# API KEY 5f9ac9efd76e998d6fc84ca0527f4e5123ec495b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
