{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from muutils.dictmagic import condense_tensor_dict\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import os  \n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import wandb\n",
    "from transformers import TrainingArguments\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: today has been such a hot topic of discussion across the Internet lately, but with an ongoing scandal that's made headlines worldwide, there's no stopping that. \"Just ask \"The X,\" among other \"hot topics\" on Reddit, over the weekend:\n",
      "\n",
      "\n",
      "Advertisement\n",
      "\n",
      "There's even conspiracy-minded Twitter users running around wondering whether an actual search result actually comes from the actual site. And\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "def generate_text(prompt, model):\n",
    "    \"\"\" Generate text using the specified model and prompt. \"\"\"\n",
    "    result = model(prompt, max_length=80, truncation=True)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"gpt2\"\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Get input from the user\n",
    "    user_input = input(\"Enter your prompt: \")\n",
    "    \n",
    "    # Generate and print the output text\n",
    "    output_text = generate_text(user_input, model)\n",
    "    print(\"Generated Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbezerra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lbezerra\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-1.3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name, trust_remote_code=True)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        root.title(\"Model Comparison Tool\")\n",
    "        name1 = \"gpt2\"\n",
    "        name2 = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "        self.model1 = load_model(name1)  \n",
    "        self.model2 = load_model(name2)  \n",
    "\n",
    "        # Setup the UI\n",
    "        tk.Label(root, text=\"Enter your prompt:\").pack()\n",
    "        \n",
    "        self.prompt_entry = tk.Entry(root, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        self.generate_button = tk.Button(root, text=\"Generate Texts\", command=self.generate_texts)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack() \n",
    "\n",
    "        self.output1_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output1_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=2).pack() \n",
    "\n",
    "        self.output2_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output2_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack()\n",
    "\n",
    "        self.select_button1 = tk.Button(root, text=\"Select Output 1\", command=lambda: self.update_model(1))\n",
    "        self.select_button1.pack()\n",
    "        self.select_button1.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.select_button2 = tk.Button(root, text=\"Select Output 2\", command=lambda: self.update_model(2))\n",
    "        self.select_button2.pack()\n",
    "        self.select_button2.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.corgis_label = tk.Label(root, text=\"Press the 'X' once you have had enough fun.\")\n",
    "        self.corgis_label.pack(side=tk.BOTTOM)\n",
    "\n",
    "    def generate_texts(self):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        result1 = self.model1(prompt, max_length=80, truncation=True)\n",
    "        result2 = self.model2(prompt, max_length=80, truncation=True)\n",
    "        self.output1 = result1[0]['generated_text']\n",
    "        self.output2 = result2[0]['generated_text']\n",
    "\n",
    "        self.output1_label.config(text=self.output1)\n",
    "        self.output2_label.config(text=self.output2)\n",
    "\n",
    "        self.select_button1.config(state=\"normal\")\n",
    "        self.select_button2.config(state=\"normal\")\n",
    "        \n",
    "    def log_user_feedback(self, prompt, selected_output, model_name):\n",
    "        feedback_data = {\n",
    "            'prompt': prompt,\n",
    "            'selected_output': selected_output,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "\n",
    "        with open('user_feedback.jsonl', 'a', encoding='utf-8') as file:\n",
    "            json.dump(feedback_data, file)\n",
    "            file.write('\\n')  # Write a newline to separate JSON objects\n",
    "\n",
    "    def update_model(self, selected_model):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        selected_output = self.output1 if selected_model == 1 else self.output2\n",
    "        model_name = \"model1\" if selected_model == 1 else \"model2\"\n",
    "        \n",
    "        self.log_user_feedback(prompt, selected_output, model_name)\n",
    "        messagebox.showinfo(\"Selection\", \"Feedback recorded! Generating new texts...\")\n",
    "        self.generate_texts()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyeasyemailforstuff\u001b[0m (\u001b[33mml_stuff\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lbezerra\\Downloads\\RLHF_mines_gpt\\wandb\\run-20240425_133629-oocylczi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml_stuff/just_work/runs/oocylczi/workspace' target=\"_blank\">running_ostrich</a></strong> to <a href='https://wandb.ai/ml_stuff/just_work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml_stuff/just_work' target=\"_blank\">https://wandb.ai/ml_stuff/just_work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml_stuff/just_work/runs/oocylczi/workspace' target=\"_blank\">https://wandb.ai/ml_stuff/just_work/runs/oocylczi/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbezerra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 11/30 [02:49<05:02, 15.89s/it]"
     ]
    }
   ],
   "source": [
    "def fine_tune_model(model_name, jsonl_file, output_dir):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"just_work\", name=\"running_ostrich\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token if undefined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    # Make sure the model's embedding sizes match if you're changing the tokenizer's pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Load dataset from JSONL file\n",
    "    dataset = load_dataset('json', data_files=jsonl_file, split='train')\n",
    "    \n",
    "    # Define preprocessing function to concatenate prompt and selected_output\n",
    "    def preprocess_function(examples):\n",
    "        texts = [p + \" \" + o for p, o in zip(examples['prompt'], examples['selected_output'])]\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Map preprocessing function\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # Data collator for padding\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define a function to compute the loss and accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "        compute_metrics=compute_metrics  # Add the compute_metrics function\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "# Example usage:\n",
    "fine_tune_model('gpt2', 'user_feedback.jsonl', 'fine_tuned_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneratorApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Text Generator\")\n",
    "\n",
    "        # Entry widget to take user input for the prompt\n",
    "        self.prompt_entry = tk.Entry(master, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        # Button to trigger text generation\n",
    "        self.generate_button = tk.Button(master, text=\"Generate\", command=self.generate_text)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        # Label to display the generated text\n",
    "        self.result_label = tk.Label(master, text=\"\", wraplength=400)\n",
    "        self.result_label.pack()\n",
    "\n",
    "    def generate_text_from_prompt(self, prompt):\n",
    "        model_dir = 'fine_tuned_model'  \n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        encoding = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask'],\n",
    "            max_length=80,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "    def generate_text(self):\n",
    "        user_input = self.prompt_entry.get()\n",
    "        result = self.generate_text_from_prompt(user_input)\n",
    "        self.result_label.config(text=result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextGeneratorApp(root)\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
