{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from muutils.dictmagic import condense_tensor_dict\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import os  \n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import json\n",
    "import wandb\n",
    "from transformers import TrainingArguments\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Today was just so far out. Maybe our favorite game was the next game.\n",
      "\n",
      "What would you consider the biggest challenges of your career?\n",
      "\n",
      "First, there haven't been any huge changes. In the future, if I wanted everything would be my own.\n",
      "\n",
      "For those of you that were born in the last 30 years, do you feel like you'd have enjoyed being an adult\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "def generate_text(prompt, model):\n",
    "    \"\"\" Generate text using the specified model and prompt. \"\"\"\n",
    "    result = model(prompt, max_length=80, truncation=True)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"gpt2\"\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Get input from the user\n",
    "    user_input = input(\"Enter your prompt: \")\n",
    "    \n",
    "    # Generate and print the output text\n",
    "    output_text = generate_text(user_input, model)\n",
    "    print(\"Generated Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\" Load a model as a text generation pipeline. \"\"\"\n",
    "    return pipeline(\"text-generation\", model=model_name, trust_remote_code=True)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        root.title(\"Model Comparison Tool\")\n",
    "        name1 = \"gpt2\"\n",
    "        name2 = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "        self.model1 = load_model(name1)  \n",
    "        self.model2 = load_model(name2)  \n",
    "\n",
    "        # Setup the UI\n",
    "        tk.Label(root, text=\"Enter your prompt:\").pack()\n",
    "        \n",
    "        self.prompt_entry = tk.Entry(root, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        self.generate_button = tk.Button(root, text=\"Generate Texts\", command=self.generate_texts)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack() \n",
    "\n",
    "        self.output1_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output1_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=2).pack() \n",
    "\n",
    "        self.output2_label = tk.Label(root, text=\"\", wraplength=300)\n",
    "        self.output2_label.pack()\n",
    "\n",
    "        tk.Label(root, text=\"\", height=1).pack()\n",
    "\n",
    "        self.select_button1 = tk.Button(root, text=\"Select Output 1\", command=lambda: self.update_model(1))\n",
    "        self.select_button1.pack()\n",
    "        self.select_button1.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.select_button2 = tk.Button(root, text=\"Select Output 2\", command=lambda: self.update_model(2))\n",
    "        self.select_button2.pack()\n",
    "        self.select_button2.config(state=\"disabled\")  # Initially disabled\n",
    "\n",
    "        self.corgis_label = tk.Label(root, text=\"Press the 'X' once you have had enough fun.\")\n",
    "        self.corgis_label.pack(side=tk.BOTTOM)\n",
    "\n",
    "    def generate_texts(self):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        result1 = self.model1(prompt, max_length=80, truncation=True)\n",
    "        result2 = self.model2(prompt, max_length=80, truncation=True)\n",
    "        self.output1 = result1[0]['generated_text']\n",
    "        self.output2 = result2[0]['generated_text']\n",
    "\n",
    "        self.output1_label.config(text=self.output1)\n",
    "        self.output2_label.config(text=self.output2)\n",
    "\n",
    "        self.select_button1.config(state=\"normal\")\n",
    "        self.select_button2.config(state=\"normal\")\n",
    "        \n",
    "    def log_user_feedback(self, prompt, selected_output, model_name):\n",
    "        feedback_data = {\n",
    "            'prompt': prompt,\n",
    "            'selected_output': selected_output,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "\n",
    "        with open('user_feedback.jsonl', 'a', encoding='utf-8') as file:\n",
    "            json.dump(feedback_data, file)\n",
    "            file.write('\\n')  # Write a newline to separate JSON objects\n",
    "\n",
    "    def update_model(self, selected_model):\n",
    "        prompt = self.prompt_entry.get()\n",
    "        selected_output = self.output1 if selected_model == 1 else self.output2\n",
    "        model_name = \"model1\" if selected_model == 1 else \"model2\"\n",
    "        \n",
    "        self.log_user_feedback(prompt, selected_output, model_name)\n",
    "        messagebox.showinfo(\"Selection\", \"Feedback recorded! Generating new texts...\")\n",
    "        self.generate_texts()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = App(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bts6rv52) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁▅█</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>████▁</td></tr><tr><td>eval/steps_per_second</td><td>████▁</td></tr><tr><td>train/epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/global_step</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.93623</td></tr><tr><td>eval/runtime</td><td>6.5717</td></tr><tr><td>eval/samples_per_second</td><td>1.674</td></tr><tr><td>eval/steps_per_second</td><td>0.304</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>75</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">running_ostrich</strong> at: <a href='https://wandb.ai/laraeseuemail/just_work/runs/bts6rv52' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work/runs/bts6rv52</a><br/> View project at: <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240506_185149-bts6rv52\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bts6rv52). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Geoff McIntyre\\RLHF_mines_gpt\\wandb\\run-20240506_185616-rpuwhlm0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laraeseuemail/just_work/runs/rpuwhlm0' target=\"_blank\">running_ostrich</a></strong> to <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laraeseuemail/just_work/runs/rpuwhlm0' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work/runs/rpuwhlm0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Geoff McIntyre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2265: UserWarning: Run (zsbwrbtq) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "  0%|          | 0/150 [05:30<?, ?it/s]\n",
      " 10%|█         | 3/30 [01:08<09:45, 21.69s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "c:\\Users\\Geoff McIntyre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2265: UserWarning: Run (bts6rv52) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "                                                \n",
      " 10%|█         | 3/30 [01:15<09:45, 21.69s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0573432445526123, 'eval_runtime': 7.092, 'eval_samples_per_second': 1.551, 'eval_steps_per_second': 0.282, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:28<09:36, 24.04s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 20%|██        | 6/30 [02:35<09:36, 24.04s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9936790466308594, 'eval_runtime': 7.053, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.284, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [03:48<08:36, 24.62s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 30%|███       | 9/30 [03:55<08:36, 24.62s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.957286834716797, 'eval_runtime': 6.9746, 'eval_samples_per_second': 1.577, 'eval_steps_per_second': 0.287, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [05:08<07:24, 24.70s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 40%|████      | 12/30 [05:15<07:24, 24.70s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9247870445251465, 'eval_runtime': 7.021, 'eval_samples_per_second': 1.567, 'eval_steps_per_second': 0.285, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [06:28<06:11, 24.76s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 50%|█████     | 15/30 [06:35<06:11, 24.76s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9053425788879395, 'eval_runtime': 7.0283, 'eval_samples_per_second': 1.565, 'eval_steps_per_second': 0.285, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [07:48<04:57, 24.79s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 60%|██████    | 18/30 [07:55<04:57, 24.79s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8993160724639893, 'eval_runtime': 7.104, 'eval_samples_per_second': 1.548, 'eval_steps_per_second': 0.282, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [09:08<03:42, 24.77s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 70%|███████   | 21/30 [09:15<03:42, 24.77s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.896510601043701, 'eval_runtime': 7.0507, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.284, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [10:28<02:28, 24.81s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 80%|████████  | 24/30 [10:35<02:28, 24.81s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.893416166305542, 'eval_runtime': 7.0035, 'eval_samples_per_second': 1.571, 'eval_steps_per_second': 0.286, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [11:39<01:07, 22.34s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      " 90%|█████████ | 27/30 [11:42<01:07, 22.34s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.891387701034546, 'eval_runtime': 3.1823, 'eval_samples_per_second': 3.457, 'eval_steps_per_second': 0.628, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [12:18<00:00, 15.53s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                                \n",
      "100%|██████████| 30/30 [12:21<00:00, 15.53s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 30/30 [12:21<00:00, 24.72s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8904223442077637, 'eval_runtime': 3.1786, 'eval_samples_per_second': 3.461, 'eval_steps_per_second': 0.629, 'epoch': 10.0}\n",
      "{'train_runtime': 741.6692, 'train_samples_per_second': 0.58, 'train_steps_per_second': 0.04, 'train_loss': 2.4366541544596356, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▄▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>████████▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁▁▁██</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁██</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.89042</td></tr><tr><td>eval/runtime</td><td>3.1786</td></tr><tr><td>eval/samples_per_second</td><td>3.461</td></tr><tr><td>eval/steps_per_second</td><td>0.629</td></tr><tr><td>total_flos</td><td>112355573760000.0</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train_loss</td><td>2.43665</td></tr><tr><td>train_runtime</td><td>741.6692</td></tr><tr><td>train_samples_per_second</td><td>0.58</td></tr><tr><td>train_steps_per_second</td><td>0.04</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">running_ostrich</strong> at: <a href='https://wandb.ai/laraeseuemail/just_work/runs/rpuwhlm0' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work/runs/rpuwhlm0</a><br/> View project at: <a href='https://wandb.ai/laraeseuemail/just_work' target=\"_blank\">https://wandb.ai/laraeseuemail/just_work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240506_185616-rpuwhlm0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fine_tune_model(model_name, jsonl_file, output_dir):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"just_work\", name=\"running_ostrich\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token if undefined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    # Make sure the model's embedding sizes match if you're changing the tokenizer's pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Load dataset from JSONL file\n",
    "    dataset = load_dataset('json', data_files=jsonl_file, split='train')\n",
    "    \n",
    "    \n",
    "    # Define preprocessing function to concatenate prompt and selected_output\n",
    "    def preprocess_function(examples):\n",
    "        texts = [p + \" \" + o for p, o in zip(examples['prompt'], examples['selected_output'])]\n",
    "        return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Map preprocessing function\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    train_test = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    # Access training and test sets\n",
    "    train_data = train_test['train']\n",
    "    test_data = train_test['test']\n",
    "\n",
    "    # Data collator for padding\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define a function to compute the loss and accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=16,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=5,\n",
    "        prediction_loss_only=True,\n",
    "        evaluation_strategy= \"epoch\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        compute_metrics=compute_metrics \n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "# Example usage:\n",
    "fine_tune_model('gpt2', 'user_feedback.jsonl', 'fine_tuned_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneratorApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Text Generator\")\n",
    "\n",
    "        # Entry widget to take user input for the prompt\n",
    "        self.prompt_entry = tk.Entry(master, width=50)\n",
    "        self.prompt_entry.pack()\n",
    "\n",
    "        # Button to trigger text generation\n",
    "        self.generate_button = tk.Button(master, text=\"Generate\", command=self.generate_text)\n",
    "        self.generate_button.pack()\n",
    "\n",
    "        # Label to display the generated text\n",
    "        self.result_label = tk.Label(master, text=\"\", wraplength=400)\n",
    "        self.result_label.pack()\n",
    "\n",
    "    def generate_text_from_prompt(self, prompt):\n",
    "        model_dir = 'fine_tuned_model'  \n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        encoding = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding['input_ids'],\n",
    "            attention_mask=encoding['attention_mask'],\n",
    "            max_length=80,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "    def generate_text(self):\n",
    "        user_input = self.prompt_entry.get()\n",
    "        result = self.generate_text_from_prompt(user_input)\n",
    "        self.result_label.config(text=result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextGeneratorApp(root)\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
